import os
import json
from openai import OpenAI
from datetime import datetime
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def read_file_content(filename):
    """Read content from a file, with error handling and logging."""
    try:
        with open(filename, 'r', encoding='utf-8') as file:
            content = file.read().strip()
            logging.info(f"Successfully read {filename}")
            logging.debug(f"Content from {filename}: {content[:100]}...")  # Log first 100 chars
            return content
    except FileNotFoundError:
        logging.error(f"Error: {filename} not found!")
        exit(1)
    except Exception as e:
        logging.error(f"Error reading {filename}: {e}")
        exit(1)

def save_to_file(content, filename):
    """Save content to a file, with error handling and logging."""
    try:
        with open(filename, 'w', encoding='utf-8') as file:
            file.write(content)
        logging.info(f"Successfully saved to {filename}")
    except Exception as e:
        logging.error(f"Error saving to {filename}: {e}")

# --- Configuration ---
# Fetch API key from environment variable
API_KEY = os.getenv('OPENAI_API_KEY')
if not API_KEY:
    print("ERROR: OPENAI_API_KEY environment variable is not set!")
    print("Please set it using: export OPENAI_API_KEY='your-api-key'")
    print("Or add it to your ~/.zshrc file for permanent setting")
    exit(1)

try:
    client = OpenAI(api_key=API_KEY)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    print("Please ensure your OPENAI_API_KEY is valid and has not expired.")
    exit(1)

# Model configuration (you can change these)
MODEL_GENERATION = "gpt-4.1-2025-04-14"       # For generating the initial response
MODEL_CRITIQUE = "gpt-4.1-2025-04-14"          # For critiquing and scoring
MODEL_REFINEMENT = "gpt-4.1-2025-04-14"        # For refining the system prompt
MODEL_CRITIQUE_REFINEMENT = "gpt-4.1-2025-04-14"  # For refining the critique prompt

# --- Helper Function to Call OpenAI Chat Completions ---
def get_llm_response(model, system_prompt, user_prompt, expect_json=False):
    """Get response from OpenAI API with error handling and logging."""
    messages = [
        {"role": "system", "content": system_prompt} if system_prompt else None,
        {"role": "user", "content": user_prompt}
    ]
    messages = [msg for msg in messages if msg is not None]

    try:
        response_format = {"type": "json_object"} if expect_json else None
        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            response_format=response_format
        )
        content = completion.choices[0].message.content.strip()
        
        if expect_json:
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            return json.loads(content.strip())
        return content
    except Exception as e:
        logging.error(f"Error calling OpenAI API ({model}): {e}")
        return None

def evaluate_critique_prompt_quality(critique_prompt, user_input, system_prompt, response, critique_data):
    """
    Evaluate the quality of the critique prompt itself by analyzing:
    1. How well it identifies key issues in the system prompt
    2. The accuracy and relevance of its scoring
    3. The actionability of its suggestions
    4. The comprehensiveness of its evaluation
    """
    meta_critique_prompt = """
You are an expert evaluator of AI critique systems. Your job is to assess how well a critique prompt performs at evaluating system prompts.

You will be given:
1. A critique prompt (the system being evaluated)
2. A user input that was given to a system
3. A system prompt that was being critiqued
4. The response generated by that system prompt
5. The critique and score produced by the critique prompt

Your task is to evaluate the critique prompt itself on these dimensions:

**Evaluation Criteria (1-100 scale):**

1. **Issue Identification Accuracy (25 points)**: How well did the critique prompt identify the real problems in the system prompt? Did it catch important issues or miss them? Did it flag non-issues as problems?

2. **Scoring Calibration (20 points)**: Is the score given appropriate for the quality of the system prompt and response? Is it well-calibrated (not too harsh or too lenient)?

3. **Actionability & Specificity (25 points)**: Are the suggestions concrete and actionable? Do they provide specific guidance on how to improve the system prompt?

4. **Comprehensiveness (15 points)**: Did the critique cover all important aspects of system prompt quality? Are there missing evaluation dimensions?

5. **Consistency & Logic (15 points)**: Is the critique internally consistent? Do the individual assessments align with the final score? Is the reasoning sound?

Provide your assessment as JSON with:
- "meta_critique": A detailed analysis of the critique prompt's performance
- "meta_score": A score from 1-100 for the critique prompt quality
- "improvement_suggestions": Specific suggestions for improving the critique prompt
"""

    evaluation_input = f"""
CRITIQUE PROMPT BEING EVALUATED:
{critique_prompt}

USER INPUT:
{user_input}

SYSTEM PROMPT:
{system_prompt}

GENERATED RESPONSE:
{response}

CRITIQUE OUTPUT:
Score: {critique_data.get('score', 'N/A')}
Critique: {critique_data.get('critique', 'N/A')}
"""

    return get_llm_response(
        MODEL_CRITIQUE_REFINEMENT,
        meta_critique_prompt,
        evaluation_input,
        expect_json=True
    )

def improve_critique_prompt(current_critique_prompt, meta_evaluation):
    """
    Improve the critique prompt based on meta-evaluation feedback.
    """
    refinement_prompt = """
You are an expert at designing critique systems for AI prompt evaluation. 

Given a critique prompt and feedback about its performance, improve the critique prompt to make it more accurate, comprehensive, and effective at evaluating system prompts.

Focus on:
1. Addressing the specific weaknesses identified in the meta-evaluation
2. Improving scoring calibration and consistency
3. Enhancing the specificity and actionability of guidance
4. Adding missing evaluation dimensions if needed
5. Maintaining the overall structure while improving effectiveness

Return only the improved critique prompt, ready to use.
"""

    improvement_input = f"""
CURRENT CRITIQUE PROMPT:
{current_critique_prompt}

META-EVALUATION FEEDBACK:
Score: {meta_evaluation.get('meta_score', 'N/A')}
Analysis: {meta_evaluation.get('meta_critique', 'N/A')}
Improvement Suggestions: {meta_evaluation.get('improvement_suggestions', 'N/A')}
"""

    return get_llm_response(
        MODEL_CRITIQUE_REFINEMENT,
        refinement_prompt,
        improvement_input
    )

# --- Enhanced Main Workflow Logic ---
def improve_system_and_critique_prompts(user_input, initial_system_prompt, initial_critique_prompt, 
                                      target_score, max_iterations, improve_critique_every=3):
    """
    Enhanced function to improve both system prompt and critique prompt iteratively.
    
    Args:
        improve_critique_every: How often to evaluate and potentially improve the critique prompt
    """
    current_system_prompt = initial_system_prompt
    current_critique_prompt = initial_critique_prompt
    current_score = 0
    iteration = 0
    history = []
    critique_improvement_history = []

    logging.info(f"Starting dual improvement process with target score: {target_score}")
    logging.info(f"Will evaluate critique prompt every {improve_critique_every} iterations")

    while current_score < target_score and iteration < max_iterations:
        iteration += 1
        logging.info(f"\n{'='*50}")
        logging.info(f"ITERATION {iteration}")
        logging.info(f"{'='*50}")

        # Generate response with current system prompt
        response = get_llm_response(MODEL_GENERATION, current_system_prompt, user_input)
        if not response:
            logging.error("Failed to get response. Aborting iteration.")
            break

        # Get critique using current critique prompt
        critique_data = get_llm_response(
            MODEL_CRITIQUE,
            current_critique_prompt,
            f"User Input: {user_input}\nSystem Prompt: {current_system_prompt}\nResponse: {response}",
            expect_json=True
        )

        if not critique_data:
            logging.error("Failed to get critique. Aborting iteration.")
            break

        current_score = int(critique_data.get("score", 0))
        critique_text = critique_data.get("critique", "No critique provided")
        
        logging.info(f"\nSYSTEM PROMPT EVALUATION:")
        logging.info(f"Score: {current_score}/100")
        logging.info(f"Critique: {critique_text}\n")

        # Evaluate and potentially improve critique prompt
        if iteration % improve_critique_every == 0 or iteration == 1:
            logging.info(f"\n--- EVALUATING CRITIQUE PROMPT (Iteration {iteration}) ---")
            
            meta_evaluation = evaluate_critique_prompt_quality(
                current_critique_prompt, user_input, current_system_prompt, 
                response, critique_data
            )
            
            if meta_evaluation:
                meta_score = meta_evaluation.get('meta_score', 0)
                meta_critique = meta_evaluation.get('meta_critique', 'No meta-critique provided')
                
                logging.info(f"CRITIQUE PROMPT EVALUATION:")
                logging.info(f"Meta-Score: {meta_score}/100")
                logging.info(f"Meta-Critique: {meta_critique}")
                
                # Improve critique prompt if meta-score is below threshold
                if meta_score < 85:  # Threshold for critique improvement
                    logging.info(f"\nImproving critique prompt (meta-score: {meta_score} < 85)")
                    improved_critique_prompt = improve_critique_prompt(current_critique_prompt, meta_evaluation)
                    
                    if improved_critique_prompt:
                        current_critique_prompt = improved_critique_prompt
                        logging.info("Critique prompt updated successfully")
                        
                        critique_improvement_history.append({
                            "iteration": iteration,
                            "meta_score": meta_score,
                            "meta_critique": meta_critique,
                            "old_critique_prompt": current_critique_prompt,
                            "new_critique_prompt": improved_critique_prompt
                        })
                else:
                    logging.info(f"Critique prompt performing well (meta-score: {meta_score} >= 85), no improvement needed")

        # Check if target reached
        if current_score >= target_score:
            logging.info(f"Target score {target_score} reached!")
            # Continue to show final results

        # Refine system prompt if needed
        if current_score < target_score and iteration < max_iterations:
            logging.info(f"\nImproving system prompt (score: {current_score} < {target_score})")
            new_system_prompt = get_llm_response(
                MODEL_REFINEMENT,
                "You are an expert at refining system prompts. Given a prompt and critique, improve it to address the identified issues while maintaining its core functionality.",
                f"Original Prompt: {current_system_prompt}\n\nCritique: {critique_text}\n\nPlease provide an improved version of the system prompt that addresses the critique points."
            )

            if new_system_prompt:
                current_system_prompt = new_system_prompt
                logging.info(f"System prompt refined successfully")

        # Record iteration history
        history.append({
            "iteration": iteration,
            "system_prompt": current_system_prompt,
            "critique_prompt_used": current_critique_prompt,
            "response": response,
            "score": current_score,
            "critique": critique_text
        })

    # Final summary
    logging.info(f"\n{'='*50}")
    logging.info("FINAL RESULTS")
    logging.info(f"{'='*50}")
    logging.info(f"Final System Prompt Score: {current_score}/100")
    logging.info(f"Total Iterations: {iteration}")
    logging.info(f"Target Score: {target_score}")
    logging.info(f"Target Score Achieved: {'Yes' if current_score >= target_score else 'No'}")
    logging.info(f"Critique Prompt Improvements: {len(critique_improvement_history)}")

    return {
        "final_system_prompt": current_system_prompt,
        "final_critique_prompt": current_critique_prompt,
        "final_score": current_score,
        "history": history,
        "critique_improvement_history": critique_improvement_history
    }

# --- Example Usage ---
if __name__ == "__main__":
    # Configuration
    MODEL_GENERATION = "o3-2025-04-16"
    MODEL_CRITIQUE = "o3-2025-04-16"
    MODEL_REFINEMENT = "o3-2025-04-16"
    MODEL_CRITIQUE_REFINEMENT = "o3-2025-04-16"
    TARGET_SCORE = 95
    MAX_ITERATIONS = 15
    IMPROVE_CRITIQUE_EVERY = 3  # Evaluate critique prompt every 3 iterations

    # Initialize OpenAI client
    API_KEY = os.getenv('OPENAI_API_KEY')
    if not API_KEY:
        logging.error("OPENAI_API_KEY not found in environment variables")
        exit(1)

    try:
        client = OpenAI(api_key=API_KEY)
    except Exception as e:
        logging.error(f"Failed to initialize OpenAI client: {e}")
        exit(1)

    # Read inputs
    user_input = read_file_content('user_input.txt')
    initial_system_prompt = read_file_content('initial_system_prompt.txt')
    initial_critique_prompt = read_file_content('critique_system_prompt.txt')

    # Run dual improvement process
    results = improve_system_and_critique_prompts(
        user_input,
        initial_system_prompt,
        initial_critique_prompt,
        TARGET_SCORE,
        MAX_ITERATIONS,
        IMPROVE_CRITIQUE_EVERY
    )

    # Save results with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save final prompts
    save_to_file(results["final_system_prompt"], f"improved_system_prompt_{timestamp}.txt")
    save_to_file(results["final_critique_prompt"], f"improved_critique_prompt_{timestamp}.txt")
    
    # Save comprehensive history
    save_to_file(json.dumps(results, indent=2), f"dual_improvement_results_{timestamp}.json")
    
    logging.info(f"\nAll results saved with timestamp: {timestamp}")
    logging.info(f"- Final system prompt: improved_system_prompt_{timestamp}.txt")
    logging.info(f"- Final critique prompt: improved_critique_prompt_{timestamp}.txt")
    logging.info(f"- Complete results: dual_improvement_results_{timestamp}.json")